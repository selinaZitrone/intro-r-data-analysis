<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>03_02_linear_models.knit</title>
    <meta charset="utf-8" />
    <meta name="author" content="Selina Baldauf" />
    <meta name="date" content="2021-08-01" />
    <script src="03_02_linear_models_files/header-attrs-2.9/header-attrs.js"></script>
    <link href="03_02_linear_models_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/new_slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: inverse title-slide
background-image: url()
background-size: 30%
background-position: 90% 90%

# Linear Models

## Introduction to R - Day 3

### Instructor: [Selina Baldauf](https://www.bcp.fu-berlin.de/biologie/arbeitsgruppen/botanik/ag_tietjen/People/wissenschaftliche_programmierer/baldauf/index.html) &lt;br&gt;

### Freie Universit√§t Berlin - Theoretical Ecology

.footnote-left[
  2021-08-01 (updated: 2021-09-01)
]

---
class: inverse, middle, center

# .large[A little bit of theoretical background]

---
# Linear models background

#### The data:

  - random sample of n data points ( `\(Y_i, X_{i1}, X_{i2},..., X_{ip}\)`, `\(i = 1...n\)`) 
  
    - `\(X_{i1}, X_{i2},..., X_{ip}\)` are p **independent predictor** variables
    
    - `\(Y_i\)` is the **dependent** observation and **response variable**

--

#### Aim of linear model:

  - (How) do the predictor variables impact the response variable?
  
  - Can we better predict values for the response variable, if we account for the predictor variables?    
  

---
# Linear models background



.pull-left[
&lt;br&gt;

**Linear regression**

- numerical response
- numerical predictor(s)


**Analysis of variance**

- numerical response
- categorical predictor(s)


**Analysis of covariance**

- numerical response
- numerical and categorical predictor(s)

]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;

]

---
# Linear models background

.left-column-large[
Linear relation with two predictors `\(X_1\)` and `\(X_2\)` ( `\(i = 1...n\)` )

**Without interaction**

`\(Y_i = \beta_0 + \beta_1*X_{1,i} + \beta_2*X_{2,i} + \epsilon\)`


**With interaction**

`\(Y_i = \beta_0 + \beta_1*X_{1,i} + \beta_2*X_{2,i} + \beta_3*X_{1,i}*X_{2,i} + \epsilon\)`

with

- `\(Y_i\)` value of response variable
- `\(\beta_0, \beta_1, \beta_2\)` model coefficients
- `\(X_{1,i}\)` value of predictor `\(X_1\)`
- `\(X_{2,i}\)` value of predictor `\(X_2\)`
- `\(\epsilon\)` error term
]

.right-column-small[
![](img/day3/lm_interaction.png)

]
---
# Goodness of fit linear model

.pull-left[

#### Model residuals

`\(Y_i - \hat{Y}_i\)`

#### Residual sum of squares (RSS)

`\(RSS = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2\)`


#### Aim of the model fitting:
Find the parameters that lead to the lowest sum of squares.
]

.pull-right[

![:scale 80%](img/day3/residual.png)
 
]

---
# Significant effects

If we account for variable A: do the residuals become significantly smaller?

.center[![:scale 70%](img/day3/lm_residuals_placeholder.png)]

---
# Assumptions for linear models

Finding the parameters that lead to the lowest residual sum of squares works only if:

- residuals are normally distributed

- residual variance is constant

- no strong outliers

--


This has to be checked for every linear model!

---
class: inverse, middle, center

# .large[How to fit linear models in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>]

---
# The data

We use productivity data from grassland sites. &lt;br&gt;
The data set is called `prod` and has  3 variables: site, productivity and richness (species richness):


```r
prod
```


```
## # A tibble: 400 x 3
##   site  richness productivity
##   &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;
## 1 site1       23         122.
## 2 site1       26         158.
## 3 site1       24         165 
## 4 site1       25         165.
## # ... with 396 more rows
```

The variable site is a factor with two levels: site1 and site2


```r
levels(prod$site) # get all levels of the factor
## [1] "site1" "site2"
```

---
class: inverse, middle, center

# .large[Linear regression in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>]

---
# Linear regression in R

.pull-left[

Is there an effect of richness on productivity?
 
#### Or in other words
 
Can we significantly increase the accuracy of productivity predictions by taking into account species richness?
]

.pull-right[

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-7-1.png" width="504" /&gt;
]

---
# Linear regression in R

Use the `lm` function to fit a linear model in R.

The general structure of the function call is like this:

.center[&lt;code&gt;&lt;b&gt;lm ( formula = .col1[Y] ~ .col2[X], data = .col3[dat] )&lt;/b&gt;&lt;/code&gt;]

with

- &lt;b&gt;.col1[Y]&lt;/b&gt; being the response variable
- &lt;b&gt;.col2[X]&lt;/b&gt; being the predictor(s)
- &lt;b&gt;.col3[dat]&lt;/b&gt; being the name of the data

--

Multiple predictors can be added with `+`, `:` or `*` depending on interaction:

- &lt;code&gt;&lt;b&gt;`Y ~ X1 + X2`&lt;/b&gt;&lt;/code&gt; tests effects without interaction

--

- &lt;code&gt;&lt;b&gt;`Y ~ X1 + X2 + X1:X2`&lt;/b&gt;&lt;/code&gt; tests single effects and interaction between `X1` and `X2`

--

- &lt;code&gt;&lt;b&gt;`Y ~ X1 * X2`&lt;/b&gt;&lt;/code&gt; is short for testing all single effects and all interactions (here same as line above)

---
# Linear regression in R

.pull-left[

Let's fit a linear model to test our hypothesis

&lt;code&gt;&lt;b&gt;lm ( formula = .col1[Y] ~ .col2[X], data = .col3[dat] )&lt;/b&gt;&lt;/code&gt;


```r
prod_lm &lt;- lm(productivity ~ richness, 
              data = prod)
```
]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-9-1.png" width="504" /&gt;
]

---
# Linear regression in R

Is the effect of richness on productivity significant?

**Or in other words**

Does the model with richness as predictor significantly reduce the residual sum of squares?

--

#### Hypothesis testing using F-Tests

Compare the complex model with a simple model that does not contain the predictor

`\(H_0\)`: The error variance in the simple model is not significantly higher than in the more complex model

--

- `\(H_0\)` accepted: simplification was justified <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> use simple model without predictor

--

- `\(H_0\)` rejected: simplification reduced explanatory power <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> use complex model with predictor

---
# Linear regression in R

#### Hypothesis testing using F-Tests in R


`\(H_0\)`: The error variance in the simple model is not significantly higher than in the more complex model

`drop1` deletes single terms from the model and performs an F-test: 


```r
drop1(prod_lm, test = "F")
## Single term deletions
## 
## Model:
## productivity ~ richness
##          Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;                132779 2326.0                      
## richness  1    254119 386899 2751.8  761.71 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Result: Reject `\(H_0\)` <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> Richness increases productivity `\((F_{1,398} = 761.71, p &lt; 0.001)\)`

---
# Extracting the coefficients

Look at model coefficients using the model `summary`


```r
summary(prod_lm)
## 
## Call:
## lm(formula = productivity ~ richness, data = prod)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.593 -13.683   0.029  13.970  42.295 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  84.6982     3.3987   24.92   &lt;2e-16 ***
## richness      2.9938     0.1085   27.60   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.27 on 398 degrees of freedom
## Multiple R-squared:  0.6568,	Adjusted R-squared:  0.6559 
## F-statistic: 761.7 on 1 and 398 DF,  p-value: &lt; 2.2e-16
```

---
# Summary table

.center[![:scale 80%](img/day3/summary_mod_resid.png)]

---
# Summary table

.center[![:scale 80%](img/day3/summary_coeffs.png)]

---
# Summary table

.center[![:scale 80%](img/day3/summary_t_stats.png)]

---
# Summary table

.center[![:scale 80%](img/day3/summary_r2.png)]

---
# Test model assumptions

Test model assumptions to make sure that:

- residuals are normally distributed
- variance is constant (homogeneous)
- there are no strong outliers or very influential observations

--

We can check that by looking at diagnostic plots


```r
# install.packages("performance")
performance::check_model(prod_lm)
```

--

Another option without an additional package using base R would be:


```r
par(mfrow = c(2,2)) # change graphics settings to fit 4 plots
plot(prod_lm) # The actual diagnostic plots
par(mfrow=c(1,1)) # change graphics settings back to default
```

---
# Test model assumptions


```r
performance::check_model(prod_lm)
```
.center[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-14-1.png" width="504" /&gt;
]
---
# Test model assmptions

.pull-left[

Check the assumptions

- residuals are normally distributed ‚úîÔ∏è
- variance is constant (homogeneous) ‚úîÔ∏è
- there are no strong outliers or very influential observations ‚úîÔ∏è 

Checking diagnostic plots needs some experience

- real life data (almost) never perfectly fit the assumptions
- linear models are to some extent robust against violations of the assumptions
]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-15-1.png" width="504" /&gt;
]

---
# Plot the model

--

#### Option 1
Extract coefficients (slope + intercept) of the model and add the regression line

--

.pull-left[


```r
# These are the coefficients of the lm
prod_lm$coefficients
## (Intercept)    richness 
##   84.698163    2.993774
```


```r
intercept &lt;- prod_lm$coefficients[1]
slope &lt;- prod_lm$coefficients[2]
```

]

---
# Plot the model

#### Option 1
Extract coefficients of the model and add the regression line

.pull-left[


```r
# These are the coefficients of the lm
prod_lm$coefficients
## (Intercept)    richness 
##   84.698163    2.993774
```


```r
intercept &lt;- prod_lm$coefficients[1]
slope &lt;- prod_lm$coefficients[2]
```

Add a line defined by slope and intercept using `geom_abline()`:


```r
 ggplot(prod, aes(x=richness, y=productivity))+
  geom_point() +
* geom_abline(slope = slope,
*             intercept = intercept)
```
]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-21-1.png" width="504" /&gt;
]

---
# Plot the model

#### Option 2
Add the regression line directly as a ggplot layer

--

.pull-left[

Use `geom_smooth()` to add the model directly:


```r
ggplot(prod, aes(x = richness, y = productivity)) +
 geom_point() +
*geom_smooth(method = "lm")
```

]

.pull-right[

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-23-1.png" width="504" /&gt;

]

---
# Plot the model

#### Option 2

Add the regression line directly as a ggplot layer

.pull-left[

Use `geom_smooth()` to add the model directly:


```r
ggplot(prod, aes(x = richness, y = productivity)) +
 geom_point() +
 geom_smooth(method = "lm")
```

Or without the confidence interval:


```r
ggplot(prod, aes(x = richness, y = productivity)) +
  geom_point() +
* geom_smooth(method = "lm", se = FALSE)
```

]

.pull-right[

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-26-1.png" width="504" /&gt;
]

---
class: inverse, center, middle

# .large[Analysis of covariance]

---
# Analysis of covariance

One categorical and one numerical predictor variable

--

.pull-left[

Two possible models:

![:scale 50%](img/day3/lm_interaction.png)

]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-27-1.png" width="504" /&gt;
]

---
# Analysis of covariance

Fit the model with `lm`

--

#### Without interaction

Species richness has an effect on productivity and there is a difference between sites.


```r
prod_lm2a &lt;- lm(productivity ~ richness + site, data = prod)
```

--

#### With interaction

Species richness has an effect on productivity and the effect differs between sites.


```r
prod_lm2b &lt;- lm(productivity ~ richness + site + richness:site, data = prod)
# or the same in short version:
# prod_lm2b &lt;- lm(productivity ~ richness * site, data = prod)
```

---
# Analysis of covariance

Are the effects significant?

#### Without interaction


```r
drop1(prod_lm2a, test = "F") # no interaction
## Single term deletions
## 
## Model:
## productivity ~ richness + site
##          Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;                 40064 1848.7                      
## richness  1     42908  82973 2137.9  425.18 &lt; 2.2e-16 ***
## site      1     92715 132779 2326.0  918.72 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--

Both, the removal of site and richness significantly increase the model's RSS <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> both site and richness are significant predictors

---
# Analysis of covariance

Are the effects significant?

#### With interaction


```r
drop1(prod_lm2b, test = "F") # interaction
## Single term deletions
## 
## Model:
## productivity ~ richness + site + richness:site
##               Df Sum of Sq   RSS    AIC F value Pr(&gt;F)
## &lt;none&gt;                     39850 1848.6               
## richness:site  1    214.53 40064 1848.7  2.1319 0.1451
```

--

The removal of the interaction does not significantly increase the model's RSS <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> the interaction is not significant and the simpler model without interaction is just as good in predicting productivity.

---
# Extracting model coefficients


```r
summary(prod_lm2a)
## 
## Call:
## lm(formula = productivity ~ richness + site, data = prod)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -30.1743  -6.2895  -0.0352   5.6955  28.9276 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 108.36550    2.02582   53.49   &lt;2e-16 ***
## richness      1.56699    0.07599   20.62   &lt;2e-16 ***
## sitesite2    38.78575    1.27962   30.31   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 10.05 on 397 degrees of freedom
## Multiple R-squared:  0.8964,	Adjusted R-squared:  0.8959 
## F-statistic:  1718 on 2 and 397 DF,  p-value: &lt; 2.2e-16
```

---
# Extracting model coefficients

![:scale 70%](img/day3/summary_ancova_coef.png)

---
# Test model assumptions

.pull-left[

Check the assumptions

- residuals are normally distributed ‚úîÔ∏è
- variance is constant (homogeneous) ‚úîÔ∏è
- there are no strong outliers or very influential observations ‚úîÔ∏è 
]

.pull-right[

```r
performance::check_model(prod_lm2a)
```

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-33-1.png" width="504" /&gt;
]

---
# Plot the model

Here, we cannot use `geom_smooth()` because it would plot the full model with interaction between the predictors

--

.pull-left[


```r
ggplot(prod, aes(x = richness, 
                 y = productivity, 
                 color = site)) +
  geom_point() +
* geom_smooth(method = "lm")
```

This plot is not appropriate if the model you present is without interaction
]
.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-35-1.png" width="504" /&gt;
]

---
# Plot the model

#### Option 1: Extract coefficients and add regression line



```r
# these are the model coefficients
prod_lm2a$coefficients
## (Intercept)    richness   sitesite2 
##  108.365497    1.566994   38.785753
```



```r
slope &lt;- prod_lm2a$coefficients[2]
intercept1 &lt;- prod_lm2a$coefficients[1]
intercept2 &lt;- prod_lm2a$coefficients[1] + prod_lm2a$coefficients[3]
```


---
# Plot the model

#### Option 1: Extract coefficients and add regression line

.pull-left[


```r
ggplot(prod, aes(x = richness, 
                 y = productivity, 
                 color = site)) +
  geom_point() +
  scale_color_manual(
    values = c("#4C7488", "#D78974")) +
* geom_abline(
*   slope = slope,
*   intercept = intercept1,
*   color = "#4C7488"
* ) +
* geom_abline(
*   slope = slope,
*   intercept = intercept2,
*   color = "#D78974"
* )
```

]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-39-1.png" width="504" /&gt;
]

---
# Plot the model

#### Option 2: Use the `predict()` function

--


```r
# step 1: create some data to predict from
pred_data &lt;- tidyr::expand_grid(
  richness = min(prod$richness):max(prod$richness), 
  site = c("site1", "site2")
)
```


```
## # A tibble: 84 x 2
##   richness site 
##      &lt;int&gt; &lt;chr&gt;
## 1       13 site1
## 2       13 site2
## 3       14 site1
## # ... with 81 more rows
```

--

- `tidyr::expand_grid` returns a tibble with all combinations of site and richness given as input
  - the `tidyr` package is part of the tidyverse
- The column names of the tibble that you create in this step have to correspond to the predictor names in the linear model

---
# Plot the model

#### Option 2: Use the `predict()` function


```r
# step 1: create some data to predict from
pred_data &lt;- tidyr::expand_grid(
  richness = min(prod$richness):max(prod$richness), 
  site = c("site1", "site2")
)
```


```r
# step2: predict productivity values from pred_data
*predictions &lt;- predict(prod_lm2a, newdata = pred_data)
```

- `predict` uses the `prod_lm2a` model and the data produced with `expand_grid` to predict productivity values for each combination of site and richness.

---
# Plot the model

#### Option 2: Use the `predict()` function


```r
# step 1: create some data to predict from
pred_data &lt;- tidyr::expand_grid(
  richness = min(prod$richness):max(prod$richness), 
  site = c("site1", "site2")
)
```


```r
# step2: predict productivity values from pred_data
predictions &lt;- predict(prod_lm2a, newdata = pred_data)
```


```r
# step 3: add predictions to the tibble
*pred_data$productivity &lt;- predictions
```

---
# Plot the model 

#### Option 2: Use the `predict()` function

.pull-left[

Add a new `geom_*()` layer to the plot from the prediction data:


```r
# step 4: Add predictions with geom_line
ggplot(prod, aes(x = richness, 
                 y = productivity, 
                 color = site)) +
  geom_point() +
* geom_line(data = pred_data)
```

- The aesthetic mapping is inherited from the top level `ggplot` call to the `geom_line`

]
.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-48-1.png" width="504" /&gt;
]

---
# Plot the model 

#### Option 2: Use the `predict()` function

** Summary of general workflow**

- **Step 1:** Use `expand_grid` to create a tibble with values for all predictor variables (the columns have to have the exact same name as the original data)

- **Step 2:** Use `predict` to predict response variable with the model using input predictor values from tibble in step 1

- **Step 3:** Add predictions as column to the tibble from step 1

- **Step 4:** Add predictions to the plot using a new `geom_line()` layer based on the tibble from step 3

---
class: inverse, middle, center

# .large[Analysis of variance (Anova)]

---
# Anova

Only categorical predictors.

--

**Example**: Data set `chickwts` about the weight of chickens fed with different diets. 

.pull-left[


```r
chickwts
```


```
## # A tibble: 71 x 2
##   weight feed     
##    &lt;dbl&gt; &lt;fct&gt;    
## 1    179 horsebean
## 2    160 horsebean
## 3    136 horsebean
## 4    227 horsebean
## # ... with 67 more rows
```
]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-51-1.png" width="504" /&gt;

]

---
# Anova

`\(H_0\)`: The diet has an effect on the weight of chicken.

--

Fit a linear model with `lm` and test the significance of the predictor:


```r
lm_chicken &lt;- lm(weight ~ feed, data = chickwts)
drop1(lm_chicken, test = "F")
## Single term deletions
## 
## Model:
## weight ~ feed
##        Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;              195556 574.39                      
## feed    5    231129 426685 619.78  15.365 5.936e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--

Removing the predictor `feed` from the model significantly increases the RSS <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg> The model with `feed` explains the data better than the model without `feed`, so the effect of `feed` on the chicken weight is significant.

---
# Extracting model coefficients


```r
summary(lm_chicken)
## 
## Call:
## lm(formula = weight ~ feed, data = chickwts)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -123.909  -34.413    1.571   38.170  103.091 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    323.583     15.834  20.436  &lt; 2e-16 ***
## feedhorsebean -163.383     23.485  -6.957 2.07e-09 ***
## feedlinseed   -104.833     22.393  -4.682 1.49e-05 ***
## feedmeatmeal   -46.674     22.896  -2.039 0.045567 *  
## feedsoybean    -77.155     21.578  -3.576 0.000665 ***
## feedsunflower    5.333     22.393   0.238 0.812495    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 54.85 on 65 degrees of freedom
## Multiple R-squared:  0.5417,	Adjusted R-squared:  0.5064 
## F-statistic: 15.36 on 5 and 65 DF,  p-value: 5.936e-10
```

---
# Extracting model coefficients

.center[![:scale 60%](img/day3/summary_anova_coefs_blank.png)]

---
# Extracting model coefficients

.center[![:scale 60%](img/day3/summary_anova_coefs.png)]

---
# Test model assumptions

.pull-left[

Check the assumptions

- residuals are normally distributed ‚úîÔ∏è
- variance is constant (homogeneous) ‚úîÔ∏è
- there are no strong outliers or very influential observations ‚úîÔ∏è 
]

.pull-right[

```r
performance::check_model(lm_chicken)
```

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-54-1.png" width="504" /&gt;
]

---
# Plot results

Plot anova results e.g. in a boxplot:

.pull-left[

```r
ggplot(chickwts, aes(feed, weight)) +
* geom_boxplot() +
  labs(x = "Diet", y= "Weight [g]")
```

]
.pull-right[

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-56-1.png" width="504" /&gt;
]

---
# Plot results

If you want a slightly nicer box plot, you could do something like this:
.pull-left[


```r
chickwts %&gt;%
  mutate(feed = as.factor(feed)) %&gt;%
  mutate(feed = fct_reorder(feed, -weight)) %&gt;%
  ggplot(aes(
    x = feed,
    y = weight,
    color = feed
  )) +
  geom_boxplot() +
  geom_point(
    size = 3, alpha = 0.25,
    position = position_jitter(width = 0.2, seed = 0)
  ) +
  coord_flip() +
  ggsci::scale_color_uchicago() +
  labs(y = "weight [g]", x = "Diet") +
  theme(legend.position = "none")
```
]
.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-58-1.png" width="504" /&gt;

]

.footnote-right[.small[Design adapted from [C√©dric Scherer]((https://www.cedricscherer.com/2019/05/17/the-evolution-of-a-ggplot-ep.-1/#sort)]]]

---
# Linear models step by step

**Step 1: Explore** the data with plots

--

**Step 2**: Write down a **question/hypothesis** and think of the **model** to test it

--

**Step 3: Fit the linear model** using the `lm` function:


```r
lm(formula = Y ~ X1 + X2 + X1:X2, data = dat)
```

- use `+` for additive effects and `:` for interactions between predictors
  
--

**Step 4: Check model assumptions** by looking at the diagnostic plots


```r
performance::check_model(mod)
```

- normally distributed residuals
- constant variance
- no strong outliers / influential data points

---
# Linear models step by step

**Step 5: Check significant variables** by conducting F-tests with `drop1`


```r
drop1(mod, test = "F")
```

- If the removal of a variable significantly increases the RSS of the model, the predictor is significant

--

**Step 6: Check effect size** e.g. in the `summary` table


```r
summary(mod)
```

- extract coefficients from the model with `mod$coefficients`

--

**Step 7: Plot model**
  - regression: regression line and scatterplot
  - analysis of covariance: regression lines and scatter plot
  - analysis of variance: boxplots, barplot, mean + sem or similar

---
# Options for model plotting

#### Option 1: `geom_smooth(method = "lm")`

- Plots a regression line with all interactions between variables
  - only use it if this is what you want to plot
  
--

#### Option 2: extract coefficients and use `geom_abline()`

- Extract slopes and intercepts from the model
  - `mod$coefficients`
- Add a `geom_abline` layer to your plot
  - `geom_abline(slope = your_slope, intercept = your_intercept)`

---
# Options for model plotting

#### Option 3: Use `predict` function

- This is the most flexible plotting option
- Step 1: Create a new tibble with data to predict from
  - Column names have to be same as predictors of linear model
  - Use e.g. `tidyr::expand_grid()` to create variable combinations
- Step 2: Predict the response for all value combinations from step 1

```r
predict(mod, newdata = pred_data)
```
- Step 3: Add predicted response to tibble from step 1
- Step 4: Add a `geom_line` layer with the new data to your plot

```r
ggplot(orig_dat, aes(x = some_x, y = some_y, color = some_col)) +
  geom_point() +
* geom_line(data = pred_data)
```

---
class: inverse, middle, center

# .large[Now you]

## Task 2: Linear models with the penugin data set

#### Find the task description &lt;a href="../01_tasks_controller.html#linear-models"&gt;here&lt;/a&gt;

---
class: inverse, center, middle

# .large[Linear models with transformed response variable]

---
# When to transform?

Transformation of the response variable Y can help with **potential violations of the model assumptions**:

- residuals non-normally distributed
  - skewed distribution
  - outliers
- trends in the residuals
- non-constant variance

--

**Example**

- Non-linear function `\(y = a * e^{b*x}\)`

- Transformed to a linear model: `\(log(y) = log(a) + b * x\)`

--

- Common natural processes: exponential growth and decay

---
# Example

Data set `decay` on the decay of soil organic matter over time.

.pull-left[
Two variables:
- `amount` of organic matter
- `time`
]
.pull-right[

&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-65-1.png" width="504" /&gt;
]

---
# Example

First, let's fit a linear model to the untransformed data and look at the diagnostic plots:

--

.pull-left[

```r
mod1 &lt;- lm(amount ~ time, data = decay)
performance::check_model(mod1)
```

Diagnostic plots look bad:

- pattern in the residuals

- skewed distribution

]
.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-67-1.png" width="504" /&gt;
]

---
# Exercise

Let's refit the model to transformed data (log-transformed response variable):

.pull-left[

```r
*mod2 &lt;- lm(log(amount) ~ time, data = decay)
performance::check_model(mod2)
```

Diagnostic plots look much better (though not perfect)

- no patterns anymore

]
.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-69-1.png" width="504" /&gt;
]

---
# Test for significant effects

Is the effect of time on the amount of organic matter significant?


```r
drop1(mod2, test = "F")
## Single term deletions
## 
## Model:
## log(amount) ~ time
##        Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    
## &lt;none&gt;               2.372 -75.678                      
## time    1    11.646 14.018 -22.602  142.39 1.038e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
# Plot the model

To plot the results of the model **on the transformed scale**, we can just use `geom_smooth` in combination with `scale_y_log10`

.pull-left[

```r
ggplot(decay, aes(x = time, y = amount)) +
  geom_point() +
* scale_y_log10() +
* geom_smooth(method = "lm")
```

]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-72-1.png" width="504" /&gt;

]

---
# Plot the model

However, we mostly want to plot the model **on the scale of the original data**. For this, we need to use `predict`

--

.pull-left[

```r
# step 1: create some data to predict from
pred_data &lt;- tibble(time = 0:30)
# step 2: make the prediction
amount_pred &lt;- predict(mod, newdata = pred_data)
# step 3: backtransform to original scale and add to prediction data
*pred_data$amount &lt;- exp(amount_pred)
# step 4: add model to plot
ggplot(decay, aes(x= time, y=amount))+
  geom_point()+
  geom_line(data = pred_data)
```

- The trick here is to **backtransform** the predictions using `exp` as the inverse function of `log`

]

.pull-right[
&lt;img src="03_02_linear_models_files/figure-html/unnamed-chunk-74-1.png" width="504" /&gt;

]

---
# Transformations step by step

1. Plot raw data

--

2. Fit a model to untransformed data

--

3. Check diagnostic plots

--

4. Apply a transformation

--

5. Re-fit model to transformed data

--

6. Check diagnostic plots again

--

7. Plot raw data and add model predictions **based on the transformed data**
  - Backtransform predictions appropriately:
    - `\(log(y) &lt;=&gt; exp(y)\)`
    - `\(sqrt(y) &lt;=&gt; y^2\)`
    
---
class: inverse, middle, center

# .large[Now you]

## Task 3: Linear models with transformations (75 min)

#### Find the task description &lt;a href="../01_tasks_controller.html#linear-models-with-transformation"&gt;here&lt;/a&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
